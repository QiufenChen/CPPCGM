{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './prot_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer_name='Rostlab/prot_bert', max_length=1024):\n",
    "        self.datasetFolderPath = 'dataset3/'\n",
    "        self.trainFilePath = os.path.join(self.datasetFolderPath, 'train.csv')\n",
    "        self.testFilePath = os.path.join(self.datasetFolderPath, 'test.csv')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "        if split==\"train\":\n",
    "            self.seqs, self.labels = self.load_dataset(self.trainFilePath)\n",
    "        else:\n",
    "            self.seqs, self.labels = self.load_dataset(self.testFilePath)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def load_dataset(self,path):\n",
    "        df = pd.read_csv(path,names=['input','labels'],skiprows=1)\n",
    "        seq = list(df['input'])\n",
    "        seq = [' '.join(i) for i in seq]\n",
    "        label = list(df['labels'])\n",
    "        assert len(seq) == len(label)\n",
    "        return seq, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepLocDataset(split=\"train\", tokenizer_name=model_name, max_length=80)\n",
    "test_dataset = DeepLocDataset(split=\"test\", tokenizer_name=model_name, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'mcc' : matthews_corrcoef(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "      return AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 20:05, Epoch 39/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.691793</td>\n",
       "      <td>0.525346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.273800</td>\n",
       "      <td>190.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.942800</td>\n",
       "      <td>0.691097</td>\n",
       "      <td>0.525346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.265600</td>\n",
       "      <td>191.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.940200</td>\n",
       "      <td>0.688716</td>\n",
       "      <td>0.578341</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.240297</td>\n",
       "      <td>2.266400</td>\n",
       "      <td>191.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.678282</td>\n",
       "      <td>0.781106</td>\n",
       "      <td>0.767726</td>\n",
       "      <td>0.773399</td>\n",
       "      <td>0.762136</td>\n",
       "      <td>0.560829</td>\n",
       "      <td>2.338000</td>\n",
       "      <td>185.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.659519</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.587628</td>\n",
       "      <td>2.208400</td>\n",
       "      <td>196.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>0.635541</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.791011</td>\n",
       "      <td>0.736402</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>2.275600</td>\n",
       "      <td>190.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.834300</td>\n",
       "      <td>0.611768</td>\n",
       "      <td>0.778802</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.825243</td>\n",
       "      <td>0.562372</td>\n",
       "      <td>2.251200</td>\n",
       "      <td>192.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.588069</td>\n",
       "      <td>0.781106</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>0.567392</td>\n",
       "      <td>2.278600</td>\n",
       "      <td>190.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.730600</td>\n",
       "      <td>0.580590</td>\n",
       "      <td>0.762673</td>\n",
       "      <td>0.771619</td>\n",
       "      <td>0.710204</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.537061</td>\n",
       "      <td>2.257400</td>\n",
       "      <td>192.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.529255</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>0.805687</td>\n",
       "      <td>0.825243</td>\n",
       "      <td>0.644835</td>\n",
       "      <td>2.362800</td>\n",
       "      <td>183.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.518962</td>\n",
       "      <td>0.808756</td>\n",
       "      <td>0.807425</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>2.423000</td>\n",
       "      <td>179.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.514226</td>\n",
       "      <td>0.799539</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.753191</td>\n",
       "      <td>0.859223</td>\n",
       "      <td>0.606149</td>\n",
       "      <td>2.342800</td>\n",
       "      <td>185.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>0.490009</td>\n",
       "      <td>0.817972</td>\n",
       "      <td>0.807786</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.805825</td>\n",
       "      <td>0.634928</td>\n",
       "      <td>2.235500</td>\n",
       "      <td>194.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.483686</td>\n",
       "      <td>0.813364</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.788018</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>0.627535</td>\n",
       "      <td>2.318600</td>\n",
       "      <td>187.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.513700</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.607818</td>\n",
       "      <td>2.291200</td>\n",
       "      <td>189.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.489500</td>\n",
       "      <td>0.492901</td>\n",
       "      <td>0.799539</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.759825</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.603580</td>\n",
       "      <td>2.247000</td>\n",
       "      <td>193.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>0.485938</td>\n",
       "      <td>0.799539</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.766816</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>0.601483</td>\n",
       "      <td>2.272500</td>\n",
       "      <td>190.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.487271</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.606417</td>\n",
       "      <td>2.235000</td>\n",
       "      <td>194.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>0.794931</td>\n",
       "      <td>0.794457</td>\n",
       "      <td>0.757709</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>2.254100</td>\n",
       "      <td>192.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.424700</td>\n",
       "      <td>0.481091</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.777273</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>0.614453</td>\n",
       "      <td>2.539900</td>\n",
       "      <td>170.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.430600</td>\n",
       "      <td>0.483714</td>\n",
       "      <td>0.804147</td>\n",
       "      <td>0.796163</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>0.805825</td>\n",
       "      <td>0.607907</td>\n",
       "      <td>3.366900</td>\n",
       "      <td>128.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.495969</td>\n",
       "      <td>0.804147</td>\n",
       "      <td>0.802784</td>\n",
       "      <td>0.768889</td>\n",
       "      <td>0.839806</td>\n",
       "      <td>0.611365</td>\n",
       "      <td>2.323300</td>\n",
       "      <td>186.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>0.491140</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.608598</td>\n",
       "      <td>2.305800</td>\n",
       "      <td>188.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.497008</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.762931</td>\n",
       "      <td>0.859223</td>\n",
       "      <td>0.618681</td>\n",
       "      <td>2.361200</td>\n",
       "      <td>183.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.504050</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.613298</td>\n",
       "      <td>2.321800</td>\n",
       "      <td>186.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.505359</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.616328</td>\n",
       "      <td>2.451700</td>\n",
       "      <td>177.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.501636</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.810185</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.625564</td>\n",
       "      <td>2.258900</td>\n",
       "      <td>192.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.495992</td>\n",
       "      <td>0.813364</td>\n",
       "      <td>0.812065</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>2.372200</td>\n",
       "      <td>182.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.508123</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.627931</td>\n",
       "      <td>2.365300</td>\n",
       "      <td>183.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.511163</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.619573</td>\n",
       "      <td>2.339900</td>\n",
       "      <td>185.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>0.486385</td>\n",
       "      <td>0.817972</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.777293</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.640550</td>\n",
       "      <td>2.355600</td>\n",
       "      <td>184.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.482777</td>\n",
       "      <td>0.820276</td>\n",
       "      <td>0.818605</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.643350</td>\n",
       "      <td>2.438500</td>\n",
       "      <td>177.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.495054</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.636333</td>\n",
       "      <td>2.288700</td>\n",
       "      <td>189.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.505005</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.627931</td>\n",
       "      <td>2.273000</td>\n",
       "      <td>190.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.512530</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.764957</td>\n",
       "      <td>0.868932</td>\n",
       "      <td>0.628830</td>\n",
       "      <td>2.294000</td>\n",
       "      <td>189.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.505763</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.627931</td>\n",
       "      <td>2.283500</td>\n",
       "      <td>190.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.330800</td>\n",
       "      <td>0.498066</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.811927</td>\n",
       "      <td>0.769565</td>\n",
       "      <td>0.859223</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>2.250100</td>\n",
       "      <td>192.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.493036</td>\n",
       "      <td>0.817972</td>\n",
       "      <td>0.817552</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.859223</td>\n",
       "      <td>0.639782</td>\n",
       "      <td>2.248600</td>\n",
       "      <td>193.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.492952</td>\n",
       "      <td>0.817972</td>\n",
       "      <td>0.816705</td>\n",
       "      <td>0.782222</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.639069</td>\n",
       "      <td>2.266400</td>\n",
       "      <td>191.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.495338</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>2.273800</td>\n",
       "      <td>190.867000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.5112057216465473, metrics={'train_runtime': 1218.2089, 'train_samples_per_second': 0.066, 'total_flos': 13889239734967680, 'epoch': 39.74})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=40,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=2,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=64,\n",
    "    fp16=True,\n",
    "    fp16_opt_level=\"02\",\n",
    "    run_name=\"ProBert-BFD-MS\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 10\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='132' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49533843994140625,\n",
       " 'eval_accuracy': 0.815668202764977,\n",
       " 'eval_f1': 0.8148148148148148,\n",
       " 'eval_precision': 0.7787610619469026,\n",
       " 'eval_recall': 0.8543689320388349,\n",
       " 'eval_mcc': 0.6348004320670899,\n",
       " 'eval_runtime': 2.2293,\n",
       " 'eval_samples_per_second': 194.682,\n",
       " 'epoch': 39.74}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('./model/bert939.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.predict(test_dataset).predictions\n",
    "golden = trainer.predict(test_dataset).label_ids\n",
    "result = torch.argmax(torch.tensor(result), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,8))\n",
    "C2= confusion_matrix(list(golden), list(result))\n",
    "sns.heatmap(C2,annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
