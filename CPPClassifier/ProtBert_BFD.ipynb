{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './prot_bert_bfd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer_name='Rostlab/prot_bert', max_length=1024):\n",
    "        self.datasetFolderPath = 'dataset/'\n",
    "        self.trainFilePath = os.path.join(self.datasetFolderPath, 'train.csv')\n",
    "        self.testFilePath = os.path.join(self.datasetFolderPath, 'test.csv')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "        if split==\"train\":\n",
    "            self.seqs, self.labels = self.load_dataset(self.trainFilePath)\n",
    "        else:\n",
    "            self.seqs, self.labels = self.load_dataset(self.testFilePath)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def load_dataset(self,path):\n",
    "        df = pd.read_csv(path,names=['input','labels'],skiprows=1)\n",
    "        seq = list(df['input'])\n",
    "        seq = [' '.join(i) for i in seq]\n",
    "        label = list(df['labels'])\n",
    "        assert len(seq) == len(label)\n",
    "        return seq, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepLocDataset(split=\"train\", tokenizer_name=model_name, max_length=80)\n",
    "test_dataset = DeepLocDataset(split=\"test\", tokenizer_name=model_name, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'mcc' : matthews_corrcoef(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "      return AutoModelForSequenceClassification.from_pretrained(model_name)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/52 13:52, Epoch 25/26]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.690898</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.375700</td>\n",
       "      <td>193.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.683549</td>\n",
       "      <td>0.845652</td>\n",
       "      <td>0.870201</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.698813</td>\n",
       "      <td>2.351900</td>\n",
       "      <td>195.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.876087</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.866920</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.750273</td>\n",
       "      <td>2.415400</td>\n",
       "      <td>190.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.578115</td>\n",
       "      <td>0.884783</td>\n",
       "      <td>0.891616</td>\n",
       "      <td>0.912134</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.769665</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>184.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.800800</td>\n",
       "      <td>0.520068</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.813412</td>\n",
       "      <td>2.305500</td>\n",
       "      <td>199.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.422432</td>\n",
       "      <td>0.878261</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.953271</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.767329</td>\n",
       "      <td>2.364400</td>\n",
       "      <td>194.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.330990</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.917695</td>\n",
       "      <td>0.944915</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.827232</td>\n",
       "      <td>2.347700</td>\n",
       "      <td>195.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.307806</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.912698</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.807068</td>\n",
       "      <td>2.312000</td>\n",
       "      <td>198.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.273028</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.953390</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.844696</td>\n",
       "      <td>2.428300</td>\n",
       "      <td>189.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.307036</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.906445</td>\n",
       "      <td>0.943723</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.807034</td>\n",
       "      <td>2.188400</td>\n",
       "      <td>210.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.289958</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.913934</td>\n",
       "      <td>0.936975</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.817958</td>\n",
       "      <td>2.195400</td>\n",
       "      <td>209.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>0.240614</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.851048</td>\n",
       "      <td>2.294700</td>\n",
       "      <td>200.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.237582</td>\n",
       "      <td>0.915217</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.952790</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.832525</td>\n",
       "      <td>2.208600</td>\n",
       "      <td>208.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.245147</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.931452</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.842438</td>\n",
       "      <td>2.193300</td>\n",
       "      <td>209.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.248440</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.843280</td>\n",
       "      <td>2.213200</td>\n",
       "      <td>207.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.250461</td>\n",
       "      <td>0.915217</td>\n",
       "      <td>0.920892</td>\n",
       "      <td>0.934156</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.829986</td>\n",
       "      <td>2.167800</td>\n",
       "      <td>212.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.236696</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.935223</td>\n",
       "      <td>0.946721</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.860425</td>\n",
       "      <td>2.201700</td>\n",
       "      <td>208.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.251731</td>\n",
       "      <td>0.932609</td>\n",
       "      <td>0.936605</td>\n",
       "      <td>0.958159</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.865755</td>\n",
       "      <td>2.194400</td>\n",
       "      <td>209.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.247777</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.935223</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>2.179500</td>\n",
       "      <td>211.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.248235</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.847161</td>\n",
       "      <td>2.201300</td>\n",
       "      <td>208.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.253588</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.934694</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.861162</td>\n",
       "      <td>2.197200</td>\n",
       "      <td>209.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.251927</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.873173</td>\n",
       "      <td>2.184700</td>\n",
       "      <td>210.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.255733</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>2.209300</td>\n",
       "      <td>208.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.270675</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.935743</td>\n",
       "      <td>0.939516</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.859949</td>\n",
       "      <td>2.185600</td>\n",
       "      <td>210.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.266484</td>\n",
       "      <td>0.932609</td>\n",
       "      <td>0.937876</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.864250</td>\n",
       "      <td>2.182100</td>\n",
       "      <td>210.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.264520</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>2.188800</td>\n",
       "      <td>210.163000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=52, training_loss=0.3542031609954742, metrics={'train_runtime': 845.6425, 'train_samples_per_second': 0.061, 'total_flos': 9520053299894400, 'epoch': 25.7})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=26,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=2,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=64,\n",
    "    fp16=True,\n",
    "    fp16_opt_level=\"02\",\n",
    "    run_name=\"ProBert-BFD-MS\",\n",
    "    seed=3407\n",
    ")\n",
    "\n",
    "# 26\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='138' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26451951265335083,\n",
       " 'eval_accuracy': 0.9347826086956522,\n",
       " 'eval_f1': 0.9397590361445783,\n",
       " 'eval_precision': 0.9435483870967742,\n",
       " 'eval_recall': 0.936,\n",
       " 'eval_mcc': 0.8687047839301032,\n",
       " 'eval_runtime': 2.203,\n",
       " 'eval_samples_per_second': 208.803,\n",
       " 'epoch': 25.7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./model/bertbfd937.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.predict(test_dataset).predictions\n",
    "golden = trainer.predict(test_dataset).label_ids\n",
    "result = torch.argmax(torch.tensor(result), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
